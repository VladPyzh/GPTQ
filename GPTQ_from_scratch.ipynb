{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrN-alQxQXhb"
      },
      "source": [
        "### GPTQ quantization implemented from scratch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xG627ZMmCyl",
        "outputId": "4e062635-5504-4611-c73f-1452b2cc6d0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.11)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.27.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.47.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2024.12.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uq3lsCr_QXhd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWdy49P4QXhd"
      },
      "source": [
        "We start with classical model load from hugging face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClNdwXbeQXhd",
        "outputId": "b4b08897-55a9-4e44-d26a-7118d9b88e56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LlamaForCausalLM(\n",
              "  (model): LlamaModel(\n",
              "    (embed_tokens): Embedding(49152, 576, padding_idx=2)\n",
              "    (layers): ModuleList(\n",
              "      (0-29): 30 x LlamaDecoderLayer(\n",
              "        (self_attn): LlamaSdpaAttention(\n",
              "          (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
              "          (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
              "          (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
              "          (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
              "          (rotary_emb): LlamaRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
              "          (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
              "          (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
              "        (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
              "      )\n",
              "    )\n",
              "    (norm): LlamaRMSNorm((576,), eps=1e-05)\n",
              "    (rotary_emb): LlamaRotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=576, out_features=49152, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "checkpoint = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
        "\n",
        "device = \"cuda:0\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForCausalLM.from_pretrained(checkpoint, torch_dtype=torch.bfloat16)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBAjuTGCQXhe"
      },
      "source": [
        "We want to create data for evaluation and computation of hessian matrix for GPTQ. I took c4 as just popular generic dataset with cleaned data. I also wrote a small function for perplexity evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6EW1LS9sQXhe",
        "outputId": "964846df-63a9-4e01-cb2c-84f8f8e45fb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/root/.cache/huggingface/modules/datasets_modules/datasets/c4/584d57ebe81c209b6c7f31727066d2c4b4bba37cb7092cdd83083d5ec11207db/c4.py:53: FutureWarning: Dataset 'c4' is deprecated and will be deleted. Use 'allenai/c4' instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "dataset = load_dataset(\"c4\", \"en\", split=\"validation\", streaming=True)\n",
        "input_texts = [s[\"text\"][:1024] for s, _ in zip(dataset, range(5000 + 128)) if s[\"text\"]!='']\n",
        "calibration_texts = input_texts[-128:]\n",
        "validation_texts = input_texts[:-128]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5uUbnNnUQXhe"
      },
      "outputs": [],
      "source": [
        "def compute_perplexity(model, tokenizer, input_texts):\n",
        "    perplexities = []\n",
        "\n",
        "    for text in tqdm(input_texts):\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
        "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
        "            loss = outputs.loss.item()\n",
        "\n",
        "        perplexities.append(np.exp(loss))\n",
        "\n",
        "    return np.mean(perplexities)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xy_pDcnHQXhe",
        "outputId": "6e2c75e4-35b2-4eb6-bb1f-258f8e6726d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5000/5000 [04:30<00:00, 18.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial model perplexity:  44.00592542669673\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "model.to(device)\n",
        "initial_res = compute_perplexity(model, tokenizer, validation_texts)\n",
        "print(\"Initial model perplexity: \", initial_res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWLnAVk-QXhf"
      },
      "source": [
        "We see that our small model has quite a good perplexity for its size. Let's check how much memory it uses and what it consists of"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QnrFPM8jQXhf",
        "outputId": "369c91d9-056f-493e-c59f-7bd498fae2bd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.269033984"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "model.get_memory_footprint() / 1e9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zzGob09JQXhf"
      },
      "outputs": [],
      "source": [
        "param_memory = 0\n",
        "\n",
        "for p in model.parameters():\n",
        "    param_memory += p.numel() * 2\n",
        "\n",
        "param_memory, buffer_memory = param_memory / 1e9, (sum(p.numel() * 4 for p in model.buffers())) / 1e9"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "param_memory + buffer_memory"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mily3KXKHeou",
        "outputId": "2fb0b5cd-438e-4f60-c96c-446d61381d35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.269033984"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SicAAM3EQXhf"
      },
      "source": [
        "Here we have 2 bytes per models parameter, as they are in bf16 and 4 bytes for buffers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mug6Fa2QQXhf"
      },
      "source": [
        "Below I implemented simple symmetrical linear quantization. It separately quantizes columns of weight matrix (columns because we transpose it before multiplication) and we will use the same technique futher for GPTQ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LN30gOkRQXhf"
      },
      "outputs": [],
      "source": [
        "class ColumnQuantizedLinear(nn.Module):\n",
        "    def __init__(self, in_features, out_features, nbits=8):\n",
        "        super().__init__()\n",
        "        self.register_buffer(\"scales\", torch.zeros(in_features, dtype=torch.bfloat16))\n",
        "        self.register_buffer(\"low_limit\", torch.tensor(- 2 ** (nbits - 1) + 1))\n",
        "        self.register_buffer(\"up_limit\", torch.tensor(2 ** (nbits - 1) - 1))\n",
        "\n",
        "        self.weight = nn.Parameter(torch.zeros((out_features, in_features), dtype=torch.int8, requires_grad=False), requires_grad=False)\n",
        "\n",
        "    def update_weight_column(self, target, idx):\n",
        "        if target.abs().max() == 0:\n",
        "            scale = 1\n",
        "        else:\n",
        "            scale = self.up_limit / target.abs().max()\n",
        "        self.scales[idx] = scale\n",
        "\n",
        "        self.weight[:, idx] = torch.clamp(torch.round(target * scale), min=self.low_limit, max=self.up_limit).to(torch.int8)\n",
        "\n",
        "    def forward(self, x):\n",
        "        weight_bf16 = self.weight.to(torch.bfloat16) / self.scales\n",
        "        return x @ weight_bf16.t()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owb7Ps0FQXhf",
        "outputId": "a752fe30-e67b-4c99-a421-972d270d4da5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "427it [00:37, 11.45it/s]\n"
          ]
        }
      ],
      "source": [
        "for name, layer in tqdm(model.named_modules()):\n",
        "    if isinstance(layer, nn.Linear):\n",
        "        out_features, in_features = layer.weight.data.shape\n",
        "        quantized_layer = ColumnQuantizedLinear(in_features, out_features).to(device)\n",
        "        for i in range(in_features):\n",
        "            quantized_layer.update_weight_column(layer.weight.data[:, i], i)\n",
        "        parent_module_name, attr_name = name.rsplit('.', 1) if '.' in name else (None, name)\n",
        "        parent_module = model if parent_module_name is None else dict(model.named_modules())[parent_module_name]\n",
        "        setattr(parent_module, attr_name, quantized_layer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8i6yL2AoQXhg",
        "outputId": "06a99741-57a4-4c82-b97b-613ab6a61f54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5000/5000 [05:21<00:00, 15.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear symmetrical quantization model perplexity:  44.55137823154533\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "linquant_res = compute_perplexity(model, tokenizer, validation_texts)\n",
        "print(\"Linear symmetrical quantization model perplexity: \", linquant_res)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we achieve nice performance, not far from the original perplexity. Let's see how much memory we need for this model. I recompute memory used for linear layers and use an uper bound for buffers' memory"
      ],
      "metadata": {
        "id": "69BDat04SCqh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_memory = 0\n",
        "\n",
        "for p in model.parameters():\n",
        "    param_memory += p.numel() * 2\n",
        "\n",
        "for module in model.modules():\n",
        "    if isinstance(module, ColumnQuantizedLinear):\n",
        "        param_memory -= module.weight.numel() * 2\n",
        "        param_memory += module.weight.numel()\n",
        "\n",
        "param_memory, buffer_memory = param_memory / 1e9, (sum(p.numel() * 4 for p in model.buffers())) / 1e9"
      ],
      "metadata": {
        "id": "OOW3YhF5Hl9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_memory + buffer_memory"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "juIkRtTCJ_an",
        "outputId": "6e54ee10-9dc6-495f-ef89-746a7abd0acb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.191780248"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arxNCgE3QXhg"
      },
      "source": [
        "Let's reload initial model and see how it would work with only 4bits:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TA_dDf5MQXhg",
        "outputId": "030d77b7-bc0e-49ae-efae-b31393b7f1e5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LlamaForCausalLM(\n",
              "  (model): LlamaModel(\n",
              "    (embed_tokens): Embedding(49152, 576, padding_idx=2)\n",
              "    (layers): ModuleList(\n",
              "      (0-29): 30 x LlamaDecoderLayer(\n",
              "        (self_attn): LlamaSdpaAttention(\n",
              "          (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
              "          (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
              "          (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
              "          (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
              "          (rotary_emb): LlamaRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
              "          (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
              "          (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
              "        (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
              "      )\n",
              "    )\n",
              "    (norm): LlamaRMSNorm((576,), eps=1e-05)\n",
              "    (rotary_emb): LlamaRotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=576, out_features=49152, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(checkpoint, torch_dtype=torch.bfloat16)\n",
        "model.eval()\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, layer in tqdm(model.named_modules()):\n",
        "    if isinstance(layer, nn.Linear):\n",
        "        out_features, in_features = layer.weight.data.shape\n",
        "        quantized_layer = ColumnQuantizedLinear(in_features, out_features, nbits=4).to(device)\n",
        "        for i in range(in_features):\n",
        "            quantized_layer.update_weight_column(layer.weight.data[:, i], i)\n",
        "        parent_module_name, attr_name = name.rsplit('.', 1) if '.' in name else (None, name)\n",
        "        parent_module = model if parent_module_name is None else dict(model.named_modules())[parent_module_name]\n",
        "        setattr(parent_module, attr_name, quantized_layer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ucw20AnV4Hr4",
        "outputId": "86d25b10-0124-43c8-836c-193f8b72c0ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "427it [00:37, 11.40it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "linquant_res = compute_perplexity(model, tokenizer, validation_texts)\n",
        "print(\"Linear symmetrical 4bit quantization model perplexity: \", linquant_res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xznbk1Sh4Kge",
        "outputId": "007e4055-3fdd-45e4-ffc0-e6cdfe95a643"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5000/5000 [05:12<00:00, 15.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear symmetrical 4bit quantization model perplexity:  1140.3871696837778\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we see much worse performance, it is seen that with this perplexity it doesn't make sense to quantize model. Let's see how much memory we win"
      ],
      "metadata": {
        "id": "yB0LqQ5ESo33"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_memory = 0\n",
        "\n",
        "for p in model.parameters():\n",
        "    param_memory += p.numel() * 2\n",
        "\n",
        "for module in model.modules():\n",
        "    if isinstance(module, ColumnQuantizedLinear):\n",
        "        param_memory -= module.weight.numel() * 2\n",
        "        param_memory += module.weight.numel() / 2\n",
        "\n",
        "param_memory, buffer_memory = param_memory / 1e9, (sum(p.numel() * 4 for p in model.buffers())) / 1e9"
      ],
      "metadata": {
        "id": "OZSzjnQRKM5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_memory + buffer_memory"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j754KlgGKKvz",
        "outputId": "895c306a-ed46-4926-b931-300a71318bff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.124540312"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's implement GPTQ model. Futher I do not compute memory savings, as they are the same, appart from the hessian matrix. But we don't need it after we do quantization, so we can omit it."
      ],
      "metadata": {
        "id": "bQShTJ4ETBHN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(checkpoint, torch_dtype=torch.bfloat16)\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCy2CqBq4Xfr",
        "outputId": "346d8706-decf-4140-904d-4c95249729a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LlamaForCausalLM(\n",
              "  (model): LlamaModel(\n",
              "    (embed_tokens): Embedding(49152, 576, padding_idx=2)\n",
              "    (layers): ModuleList(\n",
              "      (0-29): 30 x LlamaDecoderLayer(\n",
              "        (self_attn): LlamaSdpaAttention(\n",
              "          (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
              "          (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
              "          (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
              "          (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
              "          (rotary_emb): LlamaRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
              "          (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
              "          (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
              "        (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
              "      )\n",
              "    )\n",
              "    (norm): LlamaRMSNorm((576,), eps=1e-05)\n",
              "    (rotary_emb): LlamaRotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=576, out_features=49152, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jj06EjwWQXhg"
      },
      "source": [
        "Here for the GPTQ quantization we compute matrix that estimates the activation that passed to every layer. Specialy for proposes like this pytorch has `register_forward_hook` functionality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxbHtKCkQXhg",
        "outputId": "1fb4d560-ec6b-4f83-808b-e6d987f6829f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "427it [00:00, 183896.48it/s]\n",
            "100%|██████████| 128/128 [00:09<00:00, 14.20it/s]\n",
            "427it [00:00, 67492.00it/s]\n"
          ]
        }
      ],
      "source": [
        "calibration_inputs = []\n",
        "for text in calibration_texts:\n",
        "    inp = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
        "    inp = {key: value.to(device) for key, value in inp.items()}\n",
        "    calibration_inputs.append(inp)\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "hooks = []\n",
        "\n",
        "def update_hessian_hook(module, inp, out):\n",
        "    input_tensor = inp[0]\n",
        "    assert input_tensor.shape[0] == 1\n",
        "    input_tensor = input_tensor.squeeze(0)\n",
        "    if not hasattr(module, \"hessian\"):\n",
        "        module.hessian = torch.zeros((module.weight.data.shape[1], module.weight.data.shape[1]), device=device, requires_grad=False)\n",
        "\n",
        "    module.hessian += 2 * (input_tensor.t() @ input_tensor)\n",
        "\n",
        "for name, layer in tqdm(model.named_modules()):\n",
        "    if isinstance(layer, nn.Linear):\n",
        "        hooks.append(layer.register_forward_hook(update_hessian_hook))\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inp in tqdm(calibration_inputs):\n",
        "        model(**inp, labels=inp[\"input_ids\"])\n",
        "\n",
        "for name, layer in tqdm(model.named_modules()):\n",
        "    if isinstance(layer, nn.Linear):\n",
        "        layer.hessian /= len(calibration_inputs)\n",
        "\n",
        "for hook in hooks:\n",
        "    hook.remove()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gk6B585zQXhg"
      },
      "source": [
        "Here is the implementation of GPTQ quantization that updates the remaining weights to minimize the second norm of the difference in activations. To be more precise, it solves the following problem by performing quantization column by column:\n",
        "\n",
        "$$\n",
        "argmin_{\\hat{W}} ||WX - \\hat{W} X||_2^2\n",
        "$$\n",
        "\n",
        "To solve this problem, we compute hessian matrix, which is\n",
        "\n",
        "$$H =  2 X X^T$$\n",
        "\n",
        "After quantizing a column, we update the remaining matrix using the following formula:\n",
        "\n",
        "\n",
        "$$\\delta = -(w_q - quant(w_q))(H^{-1}_{qq})^{-1}(H^{-1})_{:, q}$$\n",
        "\n",
        "Since we don't want to update the hessian matrix every time using a naive inverse algorithm, the paper proposes an approximate inverse that can be computed using the formula:\n",
        "\n",
        "$$H^{-1} = H^{-1} - \\frac{H^{-1}_{:,q} H^{-1}_{q,:}}{H^{-1}_{q,q}}$$\n",
        "\n",
        "For simplicity in implementation, we do not pack 4-bit integers and save them only as 8 bits. In a real-world scenario, for memory savings, we should do so."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-uiJQCtwQXhg"
      },
      "outputs": [],
      "source": [
        "class GPTQQuantizedLinear(nn.Module):\n",
        "    def __init__(self, in_features, out_features, nbits=8):\n",
        "        super().__init__()\n",
        "        self.register_buffer(\"scales\", torch.zeros(in_features, dtype=torch.bfloat16))\n",
        "        self.register_buffer(\"low_limit\", torch.tensor(- 2 ** (nbits - 1) + 1))\n",
        "        self.register_buffer(\"up_limit\", torch.tensor(2 ** (nbits - 1) - 1))\n",
        "\n",
        "        self.weight = nn.Parameter(torch.zeros((out_features, in_features), dtype=torch.int8, requires_grad=False), requires_grad=False)\n",
        "\n",
        "    def init_weights(self, target_weight, hessian):\n",
        "        assert target_weight.shape == self.weight.shape\n",
        "\n",
        "        diag_index = torch.arange(hessian.shape[0])\n",
        "        hessian[diag_index, diag_index] += 0.1 * torch.mean(torch.diag(hessian))\n",
        "\n",
        "        invH = torch.inverse(hessian)\n",
        "\n",
        "        for idx in range(self.weight.shape[1]):\n",
        "            target = target_weight[:, idx]\n",
        "            if target.abs().max() == 0:\n",
        "                scale = 1\n",
        "            else:\n",
        "                scale = self.up_limit / target.abs().max()\n",
        "            self.scales[idx] = scale\n",
        "\n",
        "            quantized = torch.clamp(torch.round(target * scale), min=self.low_limit, max=self.up_limit).to(torch.int8)\n",
        "            self.weight[:, idx] = quantized\n",
        "            dequant = quantized.to(torch.bfloat16) / scale\n",
        "\n",
        "            delta = -(target - dequant).unsqueeze(1) / invH[idx, idx] * invH[idx, :]\n",
        "            target_weight[:, idx + 1:] += delta[:, idx + 1:]\n",
        "            invH -= (invH[:, idx].unsqueeze(1) @ invH[idx, :].unsqueeze(0)) / invH[idx, idx]\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        weight_bf16 = self.weight.to(torch.bfloat16) / self.scales\n",
        "        return x @ weight_bf16.t()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83d-ZuGQQXhg",
        "outputId": "a81e28e0-1303-4b21-b943-7ddd94bdfc26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "427it [01:25,  4.97it/s]\n"
          ]
        }
      ],
      "source": [
        "for name, layer in tqdm(model.named_modules()):\n",
        "    if isinstance(layer, nn.Linear):\n",
        "        out_features, in_features = layer.weight.data.shape\n",
        "        quantized_layer = GPTQQuantizedLinear(in_features, out_features).to(device)\n",
        "        quantized_layer.init_weights(layer.weight.data, layer.hessian)\n",
        "        parent_module_name, attr_name = name.rsplit('.', 1) if '.' in name else (None, name)\n",
        "        parent_module = model if parent_module_name is None else dict(model.named_modules())[parent_module_name]\n",
        "        setattr(parent_module, attr_name, quantized_layer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cs9ALhq4QXhh",
        "outputId": "36b5a738-960d-4f02-874c-3f001121b74d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5000/5000 [05:07<00:00, 16.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPTQ model perplexity:  44.36175512440009\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "GPTQ_res = compute_perplexity(model, tokenizer, validation_texts)\n",
        "print(\"GPTQ model perplexity: \", GPTQ_res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVC5aL3VQXhh"
      },
      "source": [
        "In comparison to our linear quantization, we see slight improvement. Let's see how our model works in 4bit settings."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(checkpoint, torch_dtype=torch.bfloat16)\n",
        "model.eval()\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1YlOHYx4a48",
        "outputId": "4b217edd-dee5-4b09-c415-3daf2d6e610d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LlamaForCausalLM(\n",
              "  (model): LlamaModel(\n",
              "    (embed_tokens): Embedding(49152, 576, padding_idx=2)\n",
              "    (layers): ModuleList(\n",
              "      (0-29): 30 x LlamaDecoderLayer(\n",
              "        (self_attn): LlamaSdpaAttention(\n",
              "          (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
              "          (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
              "          (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
              "          (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
              "          (rotary_emb): LlamaRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
              "          (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
              "          (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
              "        (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
              "      )\n",
              "    )\n",
              "    (norm): LlamaRMSNorm((576,), eps=1e-05)\n",
              "    (rotary_emb): LlamaRotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=576, out_features=49152, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, layer in tqdm(model.named_modules()):\n",
        "    if isinstance(layer, nn.Linear):\n",
        "        hooks.append(layer.register_forward_hook(update_hessian_hook))\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inp in tqdm(calibration_inputs):\n",
        "        model(**inp, labels=inp[\"input_ids\"])\n",
        "\n",
        "for name, layer in tqdm(model.named_modules()):\n",
        "    if isinstance(layer, nn.Linear):\n",
        "        layer.hessian /= len(calibration_inputs)\n",
        "\n",
        "for hook in hooks:\n",
        "    hook.remove()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "geRTBMn94jJm",
        "outputId": "11dabd36-b098-45ca-b433-351622e2ad6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "427it [00:00, 280145.13it/s]\n",
            "100%|██████████| 128/128 [00:09<00:00, 13.52it/s]\n",
            "427it [00:00, 106018.34it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, layer in tqdm(model.named_modules()):\n",
        "    if isinstance(layer, nn.Linear):\n",
        "        out_features, in_features = layer.weight.data.shape\n",
        "        quantized_layer = GPTQQuantizedLinear(in_features, out_features, nbits=4).to(device)\n",
        "        quantized_layer.init_weights(layer.weight.data, layer.hessian)\n",
        "        parent_module_name, attr_name = name.rsplit('.', 1) if '.' in name else (None, name)\n",
        "        parent_module = model if parent_module_name is None else dict(model.named_modules())[parent_module_name]\n",
        "        setattr(parent_module, attr_name, quantized_layer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMjPqZc-4cdc",
        "outputId": "12dc815c-ddcc-4773-f3e3-08bce6216418"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "427it [01:28,  4.80it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "GPTQ_res = compute_perplexity(model, tokenizer, validation_texts)\n",
        "print(\"GPTQ 4bit model perplexity: \", GPTQ_res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKKWAYqV4nk5",
        "outputId": "c77706d3-50a0-4d63-9fb8-de9dc5b8cece"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5000/5000 [05:09<00:00, 16.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPTQ 4bit model perplexity:  143.13055899315862\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, it is seen that the performance is quite far from ideal, but at the same time, it is much better than the naive approach.\n",
        "\n",
        "Possible further steps:\n",
        "\n",
        "- 1 Investigate how 4-bit quantization can be improved. Conduct testing and ensure that the method is implemented 100% correctly, as the perplexity for 4-bit quantization looks suspicious.\n",
        "- 2 For real-world scenarios, we need to control data types for buffers and possibly compute GPTQ layer by layer to avoid extra memory allocations."
      ],
      "metadata": {
        "id": "ugieI4TAVXF4"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}